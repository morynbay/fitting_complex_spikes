
import matplotlib.pyplot as plt
import pandas as pd
import sys
import numpy as np
import calcium
import scipy
from scipy.signal import find_peaks
from scipy.optimize import curve_fit
from scipy.signal import find_peaks, peak_prominences, peak_widths
import lmfit
from lmfit import Model
from lmfit import Minimizer, Parameters, report_fit
from scipy.integrate import simps
from numpy import trapz
import time
import seaborn as sns
from sklearn import datasets



# get the start time
st = time.time()

h5_file_path = '/media/bsinvivo2/team_brandon_working_copy/poste_bs_invivo1/neuron_data/2022_04_01/exp_2022-04-01_0001-0050.h5'
ca_df = calcium.load_suite2p_data_for_wavesurfer_h5(h5_file_path, plane='plane0', df_baseline=0.25,
                                                            force_redo=False, include_unclassified_cells=True,
                                                            save_pickle=False, region='dendrites',
                                                            spike_quantile=0.94)

ca_deltaf = ca_df.loc[ca_df['acquisitionNumbers'] == 1]
# ca_deltaf.plot(x='frameTimestamps', y='cell_0_fluo_chan1')
cal1 = ca_deltaf[['frameTimestamps']]     # select column with time of recording "frameTimestamps"
cal2 = ca_deltaf.filter(like='deltaf_chan1', axis=1)   # select all columns with name "cell_N_deltaf_chan1"
cal3 = ca_deltaf[['frameNumbers']]
data = pd.concat([cal1, cal2], axis=1)
col_num = cal2.shape[1]
timestamp_repeated = pd.concat([cal1]*col_num, ignore_index=True)
frameNumbers = pd.concat([cal3]*col_num, ignore_index=True)
one_column_under_another = cal2.melt()
new_data = pd.concat([frameNumbers, timestamp_repeated, one_column_under_another], axis=1)
new_data.columns = ['frameNumbers', 'frameTimestamps', 'variable', 'value']
new_data['peaks_index'] = new_data.index
tt = new_data[['frameTimestamps']]
t = tt.to_numpy().reshape(-1)
y = new_data['value'].to_numpy().reshape(-1)
pea = new_data['peaks_index'].to_numpy().reshape(-1)
peaks, properties = find_peaks(y, height=(0.1, 10), distance=8, prominence=0.2)   # distance - points between peaks
prominences = peak_prominences(y, peaks)[0]
width_half = peak_widths(y, peaks, rel_height=0.5)[0]
contour_heights = y[peaks] - prominences
all_peaks = pd.DataFrame(peaks, columns=['peaks'])
# all_peaks = pd.DataFrame(peaks)
dat_org = new_data[new_data.set_index('peaks_index').index.isin(all_peaks.set_index(['peaks']).index)].reset_index(drop=True)
promi = pd.DataFrame(prominences)
width = pd.DataFrame(width_half)
begin_promi = pd.concat([all_peaks, promi], axis=1)
exclude_low_promi = begin_promi[begin_promi.iloc[:, 1] > 0.1]
sweep_excl = exclude_low_promi.iloc[:, 0]

sweep = sweep_excl.reset_index(drop=True)

promi_last_point = promi[:-1].reset_index(drop=True)  # remove last value to fit number of rows with 'end_range'
width_last_point = width[:-1].reset_index(drop=True)  # remove last value to fit number of rows with 'end_range'
begin_range = sweep[:-1].reset_index(drop=True)       # remove last value to fit number of rows with 'end_range'
end_range = sweep[1:].reset_index(drop=True) - 5      # remove first value, and subtracted several points before peak
between_points = end_range - begin_range
r_range = pd.concat([begin_range, end_range, between_points, promi_last_point, width_last_point], axis=1)
r_range.columns = ['begin_range', 'end_range', 'between_points', 'prominence', 'width']
#   we exclude in r_range those rows which have lower than 4 points between peaks
between_peaks = pd.DataFrame(r_range.drop(r_range.index[r_range['between_points'] < 4])).reset_index(drop=True)


# ******************************** here we determine end range of fitting signal ********************************
end_spike = []
peak_ind = []
d = between_peaks.index.to_numpy()   # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  pay attention
iii = 0
while iii <= d.max():
    start = int(between_peaks.iat[iii, 0])
    stop = int(between_peaks.iat[iii, 1])
    y0n = y[start:stop]
    p0n = pea[start:stop]
    ymin = y0n.min()
    if between_peaks.iat[iii, 2] <= 4:
        end_n = stop
    # else:
    if between_peaks.iat[iii, 2] > 4:
        interva = new_data.loc[new_data['peaks_index'].between(start, stop).values, ['peaks_index', 'value']]
        end_n = interva.loc[interva['value'] == ymin, 'peaks_index'].values[0]

    end_spike.append(end_n)
    peak_ind.append((p0n))
    iii += 1

end_sig = pd.DataFrame(end_spike)
pe = pd.DataFrame(peak_ind)
signal_limits = between_peaks.copy()
signal_limits.columns = ['begin_sig', 'end_range', 'between_points', 'prominence', 'width']
signal_limits.insert(1, 'end_sig', end_sig.squeeze())
signal_limits.drop('end_range', axis=1)
r = signal_limits.index.values

#  ***********************************************************************************************************


hhh = new_data.iloc[::300, :]    # for plotting vertical lines separating each cell data
vert = hhh['frameTimestamps'].index.values

plt.plot(y)
plt.plot(peaks, y[peaks], "*")
plt.vlines(x=peaks, ymin=contour_heights, ymax=y[peaks])
plt.vlines(x=vert, ymin=-0.5, ymax=2.0, ls=":", color="green")  # drawing vertical lines
plt.show()


# def exp_func(x, exp_parameters):
#     a = exp_parameters[0]
#     b = exp_parameters[1]
#     c = exp_parameters[2]
#     return a * np.exp(-b * x) + c


# define objective function: returns the array to be minimized
def fcn2min(coeff, xn, yn):
    """Exponential Model and subtract data."""
    amp = coeff['amp']
    decay = coeff['decay']
    const = coeff['const']
    model = amp * np.exp(-xn*decay) + const
    return model - yn

# ++++++++++++++++++++++++++++++++++  first fitting  +++++++++++++++++++++++++++++++++++++++


tau_table = []
head_cols = [col for col in cal2 if 'cell_' in col]
# param_cloud = pd.DataFrame({'a': [0.8, 1.8, 5, 3], 'b': [0.1, 1.1, 5, 3], 'c': [0.3, 1.3, 5, 3]})
fit_param_table = []

# create a set of Parameters (original guess values)

params = Parameters()
params.add('amp', value=0.1, vary=True, min=0.0001)
params.add('decay', value=0.1, vary=True, min=0.01)
params.add('const', value=0.1, vary=True, min=-1)

iteration = 0
while iteration <= r.max():   # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  pay attention
    xn = t[int(between_peaks.iat[iteration, 0]):int(between_peaks.iat[iteration, 1])]
    yn = y[int(between_peaks.iat[iteration, 0]):int(between_peaks.iat[iteration, 1])]
    # do fit, here with leastsq model
    minner = Minimizer(fcn2min, params, fcn_args=(xn, yn), nan_policy='omit')
    minim_result = minner.minimize(method='leastsq')
    # calculate final result
    final = yn + minim_result.residual
    b1 = minim_result.params['decay'].value
    tau = 1 / b1
    tau_table.append(tau)
    fitted_param = [minim_result.params['amp'].value, minim_result.params['decay'].value,
                    minim_result.params['const'].value]
    fit_param_table.append(fitted_param)

    iteration += 1

promi_between_peaks = between_peaks['prominence']
width_between_peaks = between_peaks['width']
pd_param = pd.DataFrame(fit_param_table)
tau_list = pd.DataFrame(tau_table)
tau_fit_params = pd.concat([tau_list, pd_param], axis=1, ignore_index=True)
tau_fit_params.columns = ['tau1', 'a1', 'b1', 'c1']
number_tau_great_1 = tau_fit_params[abs(tau_fit_params['tau1']) > 1].count()
#  +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


#  -----------------  second fitting with another guess parameters  ---------------------
tau_fit_params_2 = tau_fit_params.reset_index(drop=False)[(tau_fit_params['b1'] <= 1)]
second_fit_indexes = tau_fit_params_2['index'].values
signal_limits_2 = signal_limits.iloc[second_fit_indexes].reset_index(drop=False)
tau_table_2 = []
fit_param_table_2 = []
r2 = signal_limits_2.index.values

# create a set of Parameters
params = Parameters()
params.add('amp', value=1.8, vary=True, min=0.0001)
params.add('decay', value=1.8, vary=True, min=0.01)
params.add('const', value=1.8, vary=True, min=-1)

iteration2 = 0
while iteration2 <= r2.max():
    xn2 = t[int(signal_limits_2.iat[iteration2, 0]):int(signal_limits_2.iat[iteration2, 1])]
    yn2 = y[int(signal_limits_2.iat[iteration2, 0]):int(signal_limits_2.iat[iteration2, 1])]
    # do fit, here with leastsq model
    minner_2 = Minimizer(fcn2min, params, fcn_args=(xn2, yn2), nan_policy='omit')
    minim_result_2 = minner_2.minimize(method='leastsq')  # (exp_func, params, args=(xn, yn), method='leastsq')
    # calculate final result
    final_2 = yn2 + minim_result_2.residual
    bb2 = minim_result_2.params['decay'].value
    tau_2 = 1 / bb2
    tau_table_2.append(tau_2)
    fitted_param_2 = [minim_result_2.params['amp'].value, minim_result_2.params['decay'].value,
                      minim_result_2.params['const'].value]
    fit_param_table_2.append(fitted_param_2)
    iteration2 += 1

pd_param_2 = pd.DataFrame(fit_param_table_2, index=second_fit_indexes)
tau_list_2 = pd.DataFrame(tau_table_2, index=second_fit_indexes)
tau_fit_params_2 = pd.concat([tau_list_2, pd_param_2], axis=1, ignore_index=False)
tau_fit_params_2.columns = ['tau2', 'a2', 'b2', 'c2']
number_tau_great_2 = tau_fit_params_2[abs(tau_fit_params_2['tau2']) > 1].count()
all_taus = pd.concat([tau_fit_params, tau_fit_params_2], axis=1, ignore_index=True)
all_taus.columns = ['tau1', 'a1', 'b1', 'c1', 'tau2', 'a2', 'b2', 'c2']
#  ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

tau_fit_params_0 = tau_fit_params.copy()
# tau_fit_params_0.drop('begin_range', axis=1)
tau_fit_params_0.loc[tau_fit_params_0.index[second_fit_indexes]] = tau_fit_params_2[:]
tau_and_prominence = pd.concat([signal_limits['begin_sig'], signal_limits['end_sig'], tau_fit_params_0,
                                promi_between_peaks, width_between_peaks], axis=1)


#  --------------------- for integration areas  ------------------------
limit_start = sweep[:-1].reset_index(drop=True) - 5    # remove last value to fit number of rows with 'end_range' and subtract several points before peaks
limit_end = sweep[1:].reset_index(drop=True) - 5
limit_between = limit_end - limit_start
limit_range = pd.concat([limit_start, limit_end, limit_between], axis=1)
limit_range.columns = ['limit_start', 'limit_end', 'limit_between']
#   we delete in r_range those rows which have lower than 4 points between peaks
limit_between = pd.DataFrame(limit_range.drop(limit_range.index[limit_range['limit_between'] < 4])).reset_index(drop=True)
limit_r = between_peaks.index.values
area_table = []
kk = 0
while kk <= limit_r.max():   # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  pay attention
    x_var = t[int(limit_between.iat[kk, 0]):int(limit_between.iat[kk, 1])]
    y_var = y[int(limit_between.iat[kk, 0]):int(limit_between.iat[kk, 1])]
    area = trapz(abs(y_var))                #  calculation of area
    area_table.append(area)
    kk += 1

area_list = pd.DataFrame(area_table)
data_tau_area_promi = pd.concat([tau_and_prominence, area_list], axis=1)
# data_tau_area_promi.drop(['a1', 'b1', 'c1'], axis=1, inplace=True)
data_tau_area_promi.columns = ['begin_sig', 'end_sig', 'tau1', 'a1', 'b1', 'c1', 'prominence', 'width', 'area']



#  ***************** calculation before, during, after stimulation   *****************************

data_before = new_data.reset_index(drop=False)[(new_data['frameTimestamps'] >= round(2.83447, 3)) &    # 86-92 frameNumbers
                                               (new_data['frameTimestamps'] <= round(3.03500, 3))]
data_before_stim = data_tau_area_promi[data_tau_area_promi['begin_sig'].isin(data_before['index'])].reset_index()
data_before_stim['stimul'] = 'before'
# data_before_stim_for_combining = between_peaks[between_peaks['begin_range'].isin(data_before['index'])].reset_index()


data_during = new_data.reset_index(drop=False)[(new_data['frameTimestamps'] >= round(3.10124, 3)) &    # 94-100 frameNumbers
                                               (new_data['frameTimestamps'] <= round(3.30152, 3))]
data_during_stim = data_tau_area_promi[data_tau_area_promi['begin_sig'].isin(data_during['index'])].reset_index()
data_during_stim['stimul'] = 'during'
# data_during_stim_for_combining = between_peaks[between_peaks['begin_range'].isin(data_during['index'])].reset_index()


data_after = new_data.reset_index(drop=False)[(new_data['frameTimestamps'] >= round(3.36801, 3)) &     # 102-108 frameNumbers
                                               (new_data['frameTimestamps'] <= round(3.6000, 3))]
data_after_stim = data_tau_area_promi[data_tau_area_promi['begin_sig'].isin(data_after['index'])].reset_index()
data_after_stim['stimul'] = 'after'
# data_after_stim_for_combining = between_peaks[between_peaks['begin_range'].isin(data_after['index'])].reset_index()

stat_during = data_during_stim.describe()
stat_before = data_before_stim.describe()
stat_after = data_after_stim.describe()
#   **********************************************************************************



spike_sum_before = []
time_before = []
yh0 = 0
xh0 = 0
hit = 0
while hit <= 29:   #  data_before_stim.index.max():   # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  pay attention
    xh = t[int(data_before_stim.iat[hit, 1]):int(data_before_stim.iat[hit, 2])]
    yh = y[int(data_before_stim.iat[hit, 1]):int(data_before_stim.iat[hit, 2])]
    # xh = t[data_before_stim_for_combining.iat[hit, 1]:data_before_stim_for_combining.iat[hit, 2]]
    # yh = y[data_before_stim_for_combining.iat[hit, 1]:data_before_stim_for_combining.iat[hit, 2]]
    yval_bef = yh0 + yh
    xval_bef = xh0 + xh
    spike_sum_before.append(yval_bef)
    time_before.append(xval_bef)
    hit += 1


spike_sum_during = []
time_during = []
yh1 = 0
xh1 = 0
hhit = 0
while hhit <= 243:   #  data_during_stim.index.max():   # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  pay attention
    xhh = t[int(data_during_stim.iat[hhit, 1]):int(data_during_stim.iat[hhit, 2])]
    yhh = y[int(data_during_stim.iat[hhit, 1]):int(data_during_stim.iat[hhit, 2])]
    # xhh = t[data_during_stim_for_combining.iat[hhit, 1]:data_during_stim_for_combining.iat[hhit, 2]]
    # yhh = y[data_during_stim_for_combining.iat[hhit, 1]:data_during_stim_for_combining.iat[hhit, 2]]
    yval_dur = yh1 + yhh
    xval_dur = xh1 + xhh
    spike_sum_during.append(yval_dur)
    time_during.append(xval_dur)
    hhit += 1


spike_sum_after = []
time_after = []
yh2 = 0
xh2 = 0
hhhit = 0
while hhhit <= 63:   #  data_after_stim.index.max():   # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  pay attention
    xhhh = t[int(data_after_stim.iat[hhhit, 1]):int(data_after_stim.iat[hhhit, 2])]
    yhhh = y[int(data_after_stim.iat[hhhit, 1]):int(data_after_stim.iat[hhhit, 2])]
    # xhhh = t[data_after_stim_for_combining.iat[hhhit, 1]:data_after_stim_for_combining.iat[hhhit, 2]]
    # yhhh = y[data_after_stim_for_combining.iat[hhhit, 1]:data_after_stim_for_combining.iat[hhhit, 2]]
    yval_aft = yh2 + yhhh
    xval_aft = xh2 + xhhh
    spike_sum_after.append(yval_aft)
    time_after.append(xval_aft)
    hhhit += 1

tim_bef = pd.DataFrame(time_before)
tim_dur = pd.DataFrame(time_during)
tim_aft = pd.DataFrame(time_after)
tim_bef_first_col = tim_bef.iloc[:, 0]
tim_dur_first_col = tim_dur.iloc[:, 0]
tim_aft_first_col = tim_aft.iloc[:, 0]
t_bef = tim_bef.subtract(tim_bef_first_col, axis=0).mean()
t_dur = tim_dur.subtract(tim_dur_first_col, axis=0).mean()
t_aft = tim_aft.subtract(tim_aft_first_col, axis=0).mean()

sp_bef = pd.DataFrame(spike_sum_before).T
sp_dur = pd.DataFrame(spike_sum_during).T
sp_aft = pd.DataFrame(spike_sum_after).T
sp_bef.plot(title='Before stimulation', legend=None)
plt.savefig('/home/murat/Desktop/Before stimulation')
sp_dur.plot(title='During stimulation', legend=None)
plt.savefig('/home/murat/Desktop/During stimulation')
sp_aft.plot(title='After stimulation', legend=None)
plt.savefig('/home/murat/Desktop/After stimulation')

# data_matrix = pd.concat([data_before_stim, data_during_stim, data_after_stim], axis=0)
# data_matrix.drop(columns=data_matrix.columns[0], axis=1, inplace=True)
# g = sns.PairGrid(data_matrix)
# # g.map(sns.scatterplot)
# g.map_diag(sns.histplot)
# g.map_offdiag(sns.scatterplot)
#


# get the end time
et = time.time()

# get the execution time
elapsed_time = et - st
print('Execution time:', elapsed_time, 'seconds')
